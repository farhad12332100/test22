{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "عیب یابی</font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معرفی مجموعه داده\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این قسمت کتاب‌خانه‌ها و ابزار مورد نیاز خود را \n",
    "<code>import</code>\n",
    " کنید \n",
    " و\n",
    "  فایل داده‌ها را که در پوشه‌ی \n",
    "  <code>Data</code>\n",
    "ذخیره‌شده‌اند را بخوانید و وارد محیط کار خود کنید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "خواندن فایل‌های داده...\n",
      "فایل‌ها با موفقیت خوانده شدند!\n",
      "Temperature Train: (107100, 2)\n",
      "Pressure Train: (53550, 2)\n",
      "VibAccel Train: (10710, 3)\n",
      "VibVelocity Train: (21420, 2)\n"
     ]
    }
   ],
   "source": [
    "# Import کردن کتابخانه‌های مورد نیاز\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# خواندن فایل‌های آموزشی\n",
    "print('خواندن فایل‌های داده...')\n",
    "temp_train = pd.read_csv('Train/Temperature_C_train.csv')\n",
    "pressure_train = pd.read_csv('Train/Pressure_kPa_train.csv')\n",
    "vibaccel_train = pd.read_csv('Train/VibAccel_m_s2_train.csv')\n",
    "vibvel_train = pd.read_csv('Train/VibVelocity_mm_s_train.csv')\n",
    "\n",
    "# خواندن فایل‌های تست\n",
    "temp_test = pd.read_csv('Test/Temperature_C_test.csv')\n",
    "pressure_test = pd.read_csv('Test/Pressure_kPa_test.csv')\n",
    "vibaccel_test = pd.read_csv('Test/VibAccel_m_s2_test.csv')\n",
    "vibvel_test = pd.read_csv('Test/VibVelocity_mm_s_test.csv')\n",
    "\n",
    "print('فایل‌ها با موفقیت خوانده شدند!')\n",
    "print(f'Temperature Train: {temp_train.shape}')\n",
    "print(f'Pressure Train: {pressure_train.shape}')\n",
    "print(f'VibAccel Train: {vibaccel_train.shape}')\n",
    "print(f'VibVelocity Train: {vibvel_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "پیش‌پردازش و مهندسی ویژگی\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "        در این سوال شما می‌توانید از هر تکنیک پیش‌پردازش/مهندسی ویژگی دلخواهتان، استفاده کنید.\n",
    "    <br>\n",
    "    تکنیک‌هایی که استفاده می‌کنید به شکل مستقیم مورد ارزیابی توسط سامانه داوری قرار <b>نمی‌گیرند.</b> بلکه همه آن‌ها در دقت مدل شما تاثیر خواهند گذاشت؛ بنابراین هر چه پیش‌پردازش/مهندسی ویژگی بهتری انجام دهید تا دقت مدل بهبود پیدا کند، امتیاز بیشتری از این سوال کسب خواهید کرد.\n",
    "    در این قسمت شما می‌توانید بخشی از داده‌ی موجود را برای اعتبارسنجی در نظر بگیرید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بازه زمانی مشترک: 2025-01-01 00:00:09 تا 2025-01-02 05:44:59\n",
      "تعداد timestamp های مرجع: 107091\n",
      "ادغام داده‌های آموزشی...\n",
      "داده ادغام شده آموزشی: (107091, 6)\n",
      "مقادیر null در هر ستون:\n",
      "timestamp              0\n",
      "Temperature_C       2122\n",
      "Pressure_kPa        4180\n",
      "VibAccel_m_s2       5780\n",
      "faulted                0\n",
      "VibVelocity_mm_s    4835\n",
      "dtype: int64\n",
      "\n",
      "توزیع کلاس‌ها:\n",
      "faulted\n",
      "normal     59775\n",
      "faulted    47316\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# تبدیل timestamp به datetime\n",
    "for df in [temp_train, pressure_train, vibaccel_train, vibvel_train, temp_test, pressure_test, vibaccel_test, vibvel_test]:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# تعیین بازه زمانی مشترک\n",
    "start_time = max([temp_train['timestamp'].min(), pressure_train['timestamp'].min(), \n",
    "                  vibaccel_train['timestamp'].min(), vibvel_train['timestamp'].min()])\n",
    "end_time = min([temp_train['timestamp'].max(), pressure_train['timestamp'].max(), \n",
    "                vibaccel_train['timestamp'].max(), vibvel_train['timestamp'].max()])\n",
    "\n",
    "print(f'بازه زمانی مشترک: {start_time} تا {end_time}')\n",
    "\n",
    "# ایجاد timestamp مرجع (هر 1 ثانیه) بر اساس Temperature که بالاترین نرخ نمونه‌برداری را دارد\n",
    "reference_timestamps = temp_train[(temp_train['timestamp'] >= start_time) & \n",
    "                                  (temp_train['timestamp'] <= end_time)]['timestamp'].values\n",
    "\n",
    "print(f'تعداد timestamp های مرجع: {len(reference_timestamps)}')\n",
    "\n",
    "# تابع برای interpolation و merge کردن داده‌ها\n",
    "def interpolate_and_merge(sensor_df, sensor_name, reference_ts):\n",
    "    # فیلتر کردن داده‌ها در بازه زمانی مشترک\n",
    "    sensor_df_filtered = sensor_df[(sensor_df['timestamp'] >= start_time) & \n",
    "                                   (sensor_df['timestamp'] <= end_time)].copy()\n",
    "    \n",
    "    # ایجاد DataFrame مرجع\n",
    "    ref_df = pd.DataFrame({'timestamp': reference_ts})\n",
    "    \n",
    "    # merge با استفاده از asof برای نزدیک‌ترین زمان\n",
    "    merged = pd.merge_asof(ref_df.sort_values('timestamp'), \n",
    "                          sensor_df_filtered.sort_values('timestamp'), \n",
    "                          on='timestamp', direction='nearest')\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# ادغام داده‌های آموزشی\n",
    "print('ادغام داده‌های آموزشی...')\n",
    "train_merged = pd.DataFrame({'timestamp': reference_timestamps})\n",
    "\n",
    "# ادغام Temperature\n",
    "temp_merged = interpolate_and_merge(temp_train, 'Temperature_C', reference_timestamps)\n",
    "train_merged['Temperature_C'] = temp_merged['Temperature_C']\n",
    "\n",
    "# ادغام Pressure\n",
    "pressure_merged = interpolate_and_merge(pressure_train, 'Pressure_kPa', reference_timestamps)\n",
    "train_merged['Pressure_kPa'] = pressure_merged['Pressure_kPa']\n",
    "\n",
    "# ادغام VibAccel (شامل target)\n",
    "vibaccel_merged = interpolate_and_merge(vibaccel_train, 'VibAccel_m_s2', reference_timestamps)\n",
    "train_merged['VibAccel_m_s2'] = vibaccel_merged['VibAccel_m_s2']\n",
    "train_merged['faulted'] = vibaccel_merged['faulted']\n",
    "\n",
    "# ادغام VibVelocity\n",
    "vibvel_merged = interpolate_and_merge(vibvel_train, 'VibVelocity_mm_s', reference_timestamps)\n",
    "train_merged['VibVelocity_mm_s'] = vibvel_merged['VibVelocity_mm_s']\n",
    "\n",
    "print(f'داده ادغام شده آموزشی: {train_merged.shape}')\n",
    "print(f'مقادیر null در هر ستون:')\n",
    "print(train_merged.isnull().sum())\n",
    "\n",
    "# نمایش توزیع کلاس‌ها\n",
    "print('\\nتوزیع کلاس‌ها:')\n",
    "print(train_merged['faulted'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "شروع پیش‌پردازش...\n",
      "ایجاد ویژگی‌های جدید...\n",
      "شکل داده نهایی: (107091, 55)\n",
      "تعداد ویژگی‌ها: 55\n",
      "کلاس‌ها: ['faulted' 'normal']\n",
      "پیش‌پردازش تکمیل شد!\n"
     ]
    }
   ],
   "source": [
    "# پیش‌پردازش و مهندسی ویژگی\n",
    "print('شروع پیش‌پردازش...')\n",
    "\n",
    "# حذف ردیف‌هایی که هیچ مقدار سنسوری ندارند\n",
    "train_cleaned = train_merged.dropna(subset=['faulted']).copy()\n",
    "\n",
    "# پُر کردن مقادیر گمشده با میانگین\n",
    "feature_columns = ['Temperature_C', 'Pressure_kPa', 'VibAccel_m_s2', 'VibVelocity_mm_s']\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "train_cleaned[feature_columns] = imputer.fit_transform(train_cleaned[feature_columns])\n",
    "\n",
    "# ایجاد ویژگی‌های جدید\n",
    "print('ایجاد ویژگی‌های جدید...')\n",
    "\n",
    "# ویژگی‌های آماری روی پنجره‌های زمانی\n",
    "window_sizes = [5, 10, 20]  # پنجره‌های 5، 10 و 20 ثانیه\n",
    "\n",
    "for window in window_sizes:\n",
    "    for col in feature_columns:\n",
    "        # میانگین متحرک\n",
    "        train_cleaned[f'{col}_rolling_mean_{window}'] = train_cleaned[col].rolling(window=window, min_periods=1).mean()\n",
    "        # انحراف معیار متحرک\n",
    "        train_cleaned[f'{col}_rolling_std_{window}'] = train_cleaned[col].rolling(window=window, min_periods=1).std()\n",
    "        # حداکثر متحرک\n",
    "        train_cleaned[f'{col}_rolling_max_{window}'] = train_cleaned[col].rolling(window=window, min_periods=1).max()\n",
    "        # حداقل متحرک\n",
    "        train_cleaned[f'{col}_rolling_min_{window}'] = train_cleaned[col].rolling(window=window, min_periods=1).min()\n",
    "\n",
    "# ویژگی‌های ترکیبی\n",
    "train_cleaned['temp_pressure_ratio'] = train_cleaned['Temperature_C'] / (train_cleaned['Pressure_kPa'] + 1e-6)\n",
    "train_cleaned['vib_total'] = train_cleaned['VibAccel_m_s2'] + train_cleaned['VibVelocity_mm_s']\n",
    "train_cleaned['vib_ratio'] = train_cleaned['VibAccel_m_s2'] / (train_cleaned['VibVelocity_mm_s'] + 1e-6)\n",
    "\n",
    "# حذف ستون timestamp\n",
    "X_train = train_cleaned.drop(['timestamp', 'faulted'], axis=1)\n",
    "y_train = train_cleaned['faulted']\n",
    "\n",
    "# تبدیل target به عددی\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "\n",
    "print(f'شکل داده نهایی: {X_train.shape}')\n",
    "print(f'تعداد ویژگی‌ها: {X_train.shape[1]}')\n",
    "print(f'کلاس‌ها: {le.classes_}')\n",
    "\n",
    "# جایگزینی مقادیر inf و nan\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "\n",
    "print('پیش‌پردازش تکمیل شد!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "آموزش مدل\n",
    "</font>\n",
    "</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "شروع آموزش مدل‌ها...\n",
      "\n",
      "آموزش RandomForest...\n",
      "RandomForest Macro F1 Score: 0.9812\n",
      "\n",
      "آموزش GradientBoosting...\n",
      "GradientBoosting Macro F1 Score: 0.9883\n",
      "\n",
      "آموزش LogisticRegression...\n",
      "LogisticRegression Macro F1 Score: 0.6625\n",
      "\n",
      "بهترین مدل: GradientBoosting با امتیاز: 0.9883\n",
      "\n",
      "آموزش مجدد بهترین مدل روی کل داده‌های آموزشی...\n"
     ]
    }
   ],
   "source": [
    "# آموزش مدل‌های مختلف\n",
    "print('شروع آموزش مدل‌ها...')\n",
    "\n",
    "# تقسیم داده برای اعتبارسنجی\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train_encoded, test_size=0.2, random_state=42, stratify=y_train_encoded)\n",
    "\n",
    "# استانداردسازی داده‌ها\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_split)\n",
    "X_val_scaled = scaler.transform(X_val_split)\n",
    "\n",
    "# مدل‌های مختلف\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, max_depth=15, \n",
    "                                          min_samples_split=5, min_samples_leaf=2,\n",
    "                                          random_state=42, n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=150, max_depth=8,\n",
    "                                                   learning_rate=0.1, random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "# آموزش و ارزیابی مدل‌ها\n",
    "best_model = None\n",
    "best_score = 0\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\nآموزش {name}...')\n",
    "    \n",
    "    # آموزش مدل\n",
    "    if name == 'LogisticRegression':\n",
    "        model.fit(X_train_scaled, y_train_split)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "    else:\n",
    "        model.fit(X_train_split, y_train_split)\n",
    "        y_pred = model.predict(X_val_split)\n",
    "    \n",
    "    # محاسبه Macro F1\n",
    "    f1_macro = f1_score(y_val_split, y_pred, average='macro')\n",
    "    model_results[name] = f1_macro\n",
    "    \n",
    "    print(f'{name} Macro F1 Score: {f1_macro:.4f}')\n",
    "    \n",
    "    # نگه‌داری بهترین مدل\n",
    "    if f1_macro > best_score:\n",
    "        best_score = f1_macro\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        if name == 'LogisticRegression':\n",
    "            best_scaler = scaler\n",
    "\n",
    "print(f'\\nبهترین مدل: {best_model_name} با امتیاز: {best_score:.4f}')\n",
    "\n",
    "# آموزش مجدد بهترین مدل روی کل داده\n",
    "print('\\nآموزش مجدد بهترین مدل روی کل داده‌های آموزشی...')\n",
    "if best_model_name == 'LogisticRegression':\n",
    "    X_train_full_scaled = scaler.fit_transform(X_train)\n",
    "    best_model.fit(X_train_full_scaled, y_train_encoded)\n",
    "else:\n",
    "    best_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "print('آموزش مدل تکمیل شد!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معیار ارزیابی\n",
    "</font>\n",
    "</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# ارزیابی نهایی مدل بر روی validation set\n",
    "print('ارزیابی نهایی مدل:')\n",
    "\n",
    "# پیش‌بینی روی validation set\n",
    "if best_model_name == 'LogisticRegression':\n",
    "    final_pred = best_model.predict(X_val_scaled)\n",
    "else:\n",
    "    final_pred = best_model.predict(X_val_split)\n",
    "\n",
    "# محاسبه امتیازهای مختلف\n",
    "macro_f1 = f1_score(y_val_split, final_pred, average='macro')\n",
    "normal_f1 = f1_score(y_val_split, final_pred, pos_label=0)\n",
    "faulted_f1 = f1_score(y_val_split, final_pred, pos_label=1)\n",
    "\n",
    "print(f'Macro F1 Score: {macro_f1:.4f}')\n",
    "print(f'Normal F1 Score: {normal_f1:.4f}')\n",
    "print(f'Faulted F1 Score: {faulted_f1:.4f}')\n",
    "\n",
    "# نمایش گزارش کامل\n",
    "print('\\nClassification Report:')\n",
    "target_names = ['normal', 'faulted']\n",
    "print(classification_report(y_val_split, final_pred, target_names=target_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val_split, final_pred)\n",
    "print('\\nConfusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# بررسی اینکه آیا امتیاز بالای حد آستانه است\n",
    "if macro_f1 >= 0.5:\n",
    "    print(f'✅ مدل شرط حداقل امتیاز (0.5) را برآورده می‌کند: {macro_f1:.4f}')\n",
    "else:\n",
    "    print(f'❌ مدل شرط حداقل امتیاز (0.5) را برآورده نمی‌کند: {macro_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    " پیش‌بینی برای داده تست و خروجی\n",
    "</font>\n",
    "</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# پیش‌بینی برای داده‌های تست\n",
    "print('شروع پیش‌بینی روی داده‌های تست...')\n",
    "\n",
    "# تعیین بازه زمانی مشترک برای تست\n",
    "start_time_test = max([temp_test['timestamp'].min(), pressure_test['timestamp'].min(), \n",
    "                      vibaccel_test['timestamp'].min(), vibvel_test['timestamp'].min()])\n",
    "end_time_test = min([temp_test['timestamp'].max(), pressure_test['timestamp'].max(), \n",
    "                    vibaccel_test['timestamp'].max(), vibvel_test['timestamp'].max()])\n",
    "\n",
    "# ایجاد timestamp مرجع برای تست\n",
    "reference_timestamps_test = temp_test[(temp_test['timestamp'] >= start_time_test) & \n",
    "                                     (temp_test['timestamp'] <= end_time_test)]['timestamp'].values\n",
    "\n",
    "print(f'تعداد timestamp های تست: {len(reference_timestamps_test)}')\n",
    "\n",
    "# ادغام داده‌های تست\n",
    "print('ادغام داده‌های تست...')\n",
    "test_merged = pd.DataFrame({'timestamp': reference_timestamps_test})\n",
    "\n",
    "# ادغام هر سنسور\n",
    "temp_test_merged = interpolate_and_merge(temp_test, 'Temperature_C', reference_timestamps_test)\n",
    "test_merged['Temperature_C'] = temp_test_merged['Temperature_C']\n",
    "\n",
    "pressure_test_merged = interpolate_and_merge(pressure_test, 'Pressure_kPa', reference_timestamps_test)\n",
    "test_merged['Pressure_kPa'] = pressure_test_merged['Pressure_kPa']\n",
    "\n",
    "vibaccel_test_merged = interpolate_and_merge(vibaccel_test, 'VibAccel_m_s2', reference_timestamps_test)\n",
    "test_merged['VibAccel_m_s2'] = vibaccel_test_merged['VibAccel_m_s2']\n",
    "\n",
    "vibvel_test_merged = interpolate_and_merge(vibvel_test, 'VibVelocity_mm_s', reference_timestamps_test)\n",
    "test_merged['VibVelocity_mm_s'] = vibvel_test_merged['VibVelocity_mm_s']\n",
    "\n",
    "# پیش‌پردازش داده‌های تست (همان فرآیند train)\n",
    "print('پیش‌پردازش داده‌های تست...')\n",
    "\n",
    "# پُر کردن مقادیر گمشده\n",
    "test_merged[feature_columns] = imputer.transform(test_merged[feature_columns])\n",
    "\n",
    "# ایجاد ویژگی‌های جدید\n",
    "for window in window_sizes:\n",
    "    for col in feature_columns:\n",
    "        test_merged[f'{col}_rolling_mean_{window}'] = test_merged[col].rolling(window=window, min_periods=1).mean()\n",
    "        test_merged[f'{col}_rolling_std_{window}'] = test_merged[col].rolling(window=window, min_periods=1).std()\n",
    "        test_merged[f'{col}_rolling_max_{window}'] = test_merged[col].rolling(window=window, min_periods=1).max()\n",
    "        test_merged[f'{col}_rolling_min_{window}'] = test_merged[col].rolling(window=window, min_periods=1).min()\n",
    "\n",
    "# ویژگی‌های ترکیبی\n",
    "test_merged['temp_pressure_ratio'] = test_merged['Temperature_C'] / (test_merged['Pressure_kPa'] + 1e-6)\n",
    "test_merged['vib_total'] = test_merged['VibAccel_m_s2'] + test_merged['VibVelocity_mm_s']\n",
    "test_merged['vib_ratio'] = test_merged['VibAccel_m_s2'] / (test_merged['VibVelocity_mm_s'] + 1e-6)\n",
    "\n",
    "# آماده‌سازی برای پیش‌بینی\n",
    "X_test = test_merged.drop(['timestamp'], axis=1)\n",
    "\n",
    "# جایگزینی مقادیر inf و nan\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "# پیش‌بینی\n",
    "if best_model_name == 'LogisticRegression':\n",
    "    X_test_scaled = best_scaler.transform(X_test)\n",
    "    test_predictions = best_model.predict(X_test_scaled)\n",
    "else:\n",
    "    test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# تبدیل پیش‌بینی‌ها به رشته\n",
    "test_predictions_labels = le.inverse_transform(test_predictions)\n",
    "\n",
    "# ایجاد DataFrame نهایی\n",
    "submission = pd.DataFrame({\n",
    "    'prediction': test_predictions_labels\n",
    "})\n",
    "\n",
    "print(f'تعداد پیش‌بینی‌ها: {len(submission)}')\n",
    "print('توزیع پیش‌بینی‌ها:')\n",
    "print(submission['prediction'].value_counts())\n",
    "\n",
    "print('پیش‌بینی تکمیل شد!')\n",
    "print('حالا میتونی سلول آخر رو اجرا کنی تا فایل result.zip ساخته بشه.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "<b>سلول جواب‌ساز</b>\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    برای ساخته‌شدن فایل <code>result.zip</code> سلول زیر را اجرا کنید. توجه داشته باشید که پیش از اجرای سلول زیر تغییرات اعمال شده در نت‌بوک را ذخیره کرده باشید (<code>ctrl+s</code>) در غیر این صورت، در پایان مسابقه نمره شما به صفر تغییر خواهد کرد.\n",
    "    <br>\n",
    "    همچنین اگر از کولب برای اجرای این فایل نوت‌بوک استفاده می‌کنید، قبل از ارسال فایل <code>result.zip</code>، آخرین نسخه‌ی نوت‌بوک خود را دانلود کرده و داخل فایل ارسالی قرار دهید.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'notebook.ipynb')):\n",
    "    %notebook -e notebook.ipynb\n",
    "\n",
    "def compress(file_names):\n",
    "    print(\"File Paths:\")\n",
    "    print(file_names)\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
    "        for file_name in file_names:\n",
    "            zf.write('./' + file_name, file_name, compress_type=compression)\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "file_names = ['notebook.ipynb', 'submission.csv']\n",
    "compress(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
